{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466204de",
   "metadata": {},
   "source": [
    "# The Lazy Book Report\n",
    "\n",
    "Your professor has assigned a book report on \"The Red-Headed League\" by Arthur Conan Doyle. \n",
    "\n",
    "You haven't read the book. And out of stubbornness, you won't.\n",
    "\n",
    "But you *have* learned NLP. Let's use it to answer the professor's questions without reading.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's fetch the text from Project Gutenberg and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a46884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story loaded: 4000 words in 3 sections\n",
      "Section sizes: [1333, 1333, 1334]\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare text - RUN THIS CELL FIRST\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Python-urllib'})\n",
    "with urllib.request.urlopen(req, timeout=30) as resp:\n",
    "    text = resp.read().decode('utf-8')\n",
    "\n",
    "# Strip Gutenberg boilerplate\n",
    "text = text.split('*** START OF')[1].split('***')[1]\n",
    "text = text.split('*** END OF')[0]\n",
    "\n",
    "# Extract \"The Red-Headed League\" story (it's the second story in the collection)\n",
    "matches = list(re.finditer(r'THE RED-HEADED LEAGUE', text, re.IGNORECASE))\n",
    "story_start = matches[1].end()\n",
    "story_text = text[story_start:]\n",
    "story_end = re.search(r'\\n\\s*III\\.\\s*\\n', story_text)\n",
    "story_text = story_text[:story_end.start()] if story_end else story_text\n",
    "\n",
    "# Split into 3 sections by word count\n",
    "words = story_text.split()[:4000]\n",
    "section_size = len(words) // 3\n",
    "sections = [\n",
    "    ' '.join(words[:section_size]),\n",
    "    ' '.join(words[section_size:2*section_size]),\n",
    "    ' '.join(words[2*section_size:])\n",
    "]\n",
    "\n",
    "print(f\"Story loaded: {len(words)} words in {len(sections)} sections\")\n",
    "print(f\"Section sizes: {[len(s.split()) for s in sections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e757a",
   "metadata": {},
   "source": [
    "## Professor's Questions\n",
    "\n",
    "Your professor wants you to answer 5 questions about the story. Let's use NLP to find the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: Writing Style\n",
    "\n",
    "> \"This text is from the 1890s. What makes it different from modern writing?\"\n",
    "\n",
    "**NLP Method:** Use preprocessing to compute text statistics. Tokenize the text and calculate:\n",
    "- Vocabulary richness (unique words / total words)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "\n",
    "**Hint:** Formal, literary writing typically shows higher vocabulary richness and longer sentences than modern casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1710e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary richness: 0.082\n",
      "Average sentence length: 15.24 words\n",
      "Average word length: 4.08 chars\n"
     ]
    }
   ],
   "source": [
    "# Your code here: compute text statistics\n",
    "# You'll need: import string, import re\n",
    "import string\n",
    "\n",
    "#nltk.download('punkt_tab')\n",
    "# - Tokenize: remove punctuation, lowercase\n",
    "abbreviations = ['Mr.', 'Mrs.', 'Dr.', 'St.', 'etc.']\n",
    "text_flag = story_text\n",
    "for abbr in abbreviations:\n",
    "    text_flag = text_flag.replace(abbr, abbr.replace('.', 'PROT'))\n",
    "\n",
    "# Split on sentence-ending punctuation\n",
    "sentences = re.findall(r'[^.!?]+[.!?]', text_flag)\n",
    "# Restore abbreviation periods and strip whitespace\n",
    "sentences_vector = [\n",
    "    s.replace('PROT', '.').strip()\n",
    "    for s in sentences\n",
    "    if s.strip()\n",
    "]\n",
    "\n",
    "clean_text = story_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "tokens = [word for s in sentences_vector for word in re.findall(r'[a-z]+', s)]\n",
    "\n",
    "words_only = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "vocab_richness = len(set(words_only)) / len(words_only)\n",
    "avg_sentence_length = len(tokens) / len(sentences)\n",
    "avg_word_length = sum(len(w) for w in tokens) / len(tokens)\n",
    "\n",
    "print(f\"Vocabulary richness: {vocab_richness:.3f}\")\n",
    "print(f\"Average sentence length: {avg_sentence_length:.2f} words\")\n",
    "print(f\"Average word length: {avg_word_length:.2f} chars\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Main Characters\n",
    "\n",
    "> \"Who are the main characters in this story?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract PERSON entities.\n",
    "\n",
    "**Hint:** Use spaCy's `en_core_web_sm` model. Process the text and filter entities where `ent.label_ == 'PERSON'`. Count how often each name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332a59da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes: 392\n",
      "Watson: 71\n",
      "Lestrade: 38\n",
      "Rucastle: 33\n",
      "McCarthy: 32\n",
      "Arthur: 20\n",
      "Hunter: 19\n",
      "Frank: 18\n",
      "Sherlock Holmes: 15\n",
      "Wilson: 13\n",
      "Hosmer Angel: 13\n",
      "Windibank: 13\n",
      "Merryweather: 12\n",
      "Holder: 12\n",
      "Peterson: 11\n",
      "Roylott: 11\n",
      "Miss Stoner: 11\n",
      "Jones: 10\n",
      "Turner: 10\n",
      "Horner: 10\n",
      "Mary: 10\n",
      "Alice: 9\n",
      "Henry Baker: 9\n",
      "Jabez Wilson: 8\n",
      "Hatherley: 8\n",
      "Horsham: 8\n",
      "Neville St. Clair: 8\n",
      "Simon: 8\n",
      "Duncan Ross: 7\n",
      "Bradstreet: 7\n",
      "Oakshott: 7\n",
      "Hosmer: 6\n",
      "Angel: 6\n",
      "Ross: 6\n",
      "James: 6\n",
      "Lee: 6\n",
      "Baker: 6\n",
      "Stoke Moran: 6\n",
      "Lysander Stark: 6\n",
      "George Burnwell: 6\n",
      "Mary Sutherland: 5\n",
      "John Clay: 5\n",
      "James Windibank: 5\n",
      "Surrey: 5\n",
      "Alpha: 5\n",
      "Grimesby Roylott: 5\n",
      "Stoper: 5\n",
      "Fowler: 5\n",
      "Vincent Spaulding: 4\n",
      "James McCarthy: 4\n",
      "John Openshaw: 4\n",
      "Hudson: 4\n",
      "Waterloo: 4\n",
      "Pondicherry: 4\n",
      "Swandam Lane: 4\n",
      "Aloysius Doran: 4\n",
      "Flora Millar: 4\n",
      "Toller: 4\n",
      "Albert: 3\n",
      "Irene Adler: 3\n",
      "William Crowder: 3\n",
      "John: 3\n",
      "Openshaw: 3\n",
      "Kate: 3\n",
      "Kent: 3\n",
      "Hugh Boone: 3\n",
      "Jem: 3\n",
      "Ferguson: 3\n",
      "Backwater: 3\n",
      "Robert St. Simon: 3\n",
      "Hatty Doran: 3\n",
      "Doran: 3\n",
      "Flora: 3\n",
      "I. ‘: 2\n",
      "William Morris: 2\n",
      "Jump: 2\n",
      "Hardy: 2\n",
      "Miss Sutherland: 2\n",
      "Jove: 2\n",
      "McCarthys: 2\n",
      "Boscombe Pool: 2\n",
      "Fordham: 2\n",
      "K. K. K.: 2\n",
      "McCauley: 2\n",
      "John Swain: 2\n",
      "Isa Whitney: 2\n",
      "John Horner: 2\n",
      "James Ryder: 2\n",
      "Windigate: 2\n",
      "Kilburn: 2\n",
      "Maggie: 2\n",
      "Helen Stoner: 2\n",
      "Stoner: 2\n",
      "Julia: 2\n",
      "Miss: 2\n",
      "Roylott’s: 2\n",
      "Grimesby Roylott’s: 2\n",
      "Stoke\n",
      "Moran: 2\n",
      "Stark: 2\n",
      "Becher: 2\n",
      "Lady St. Simon: 2\n",
      "Robert: 2\n",
      "Moulton: 2\n",
      "Lucy Parr: 2\n",
      "Fairbank: 2\n",
      "Spence Munro: 2\n",
      "Bradshaw: 2\n",
      "obese: 1\n",
      "baggy grey: 1\n",
      "Freemason: 1\n",
      "Ezekiah Hopkins: 1\n",
      "Vincent\n",
      "Spaulding: 1\n",
      "Armour: 1\n",
      "Edward Street: 1\n",
      "Mortimer’s: 1\n",
      "McFarlane: 1\n",
      "James’s Hall: 1\n",
      "Peter\n",
      "Jones: 1\n",
      "Sholto: 1\n",
      "Agra: 1\n",
      "Eton: 1\n",
      "Great Scott: 1\n",
      "John\n",
      "Clay: 1\n",
      "I.\n",
      "\n",
      ": 1\n",
      "L’homme c’est: 1\n",
      "Gustave Flaubert: 1\n",
      "George Sand: 1\n",
      "Dundas: 1\n",
      "Etherege: 1\n",
      "Sherlock\n",
      "Holmes: 1\n",
      "Mary\n",
      "Sutherland: 1\n",
      "Ned: 1\n",
      "Lyon Place: 1\n",
      "grey Harris: 1\n",
      "Balzac: 1\n",
      "Miss\n",
      "Sutherland: 1\n",
      "gaunt: 1\n",
      "John\n",
      "Turner: 1\n",
      "Charles McCarthy: 1\n",
      "Patience Moran: 1\n",
      "Bristol: 1\n",
      "John\n",
      "Cobb: 1\n",
      "Petrarch: 1\n",
      "Stroud Valley: 1\n",
      "Willows: 1\n",
      "Hereford: 1\n",
      "Mark: 1\n",
      "George Meredith: 1\n",
      "Square: 1\n",
      "Moran: 1\n",
      "John Turner: 1\n",
      "bush: 1\n",
      "Jack: 1\n",
      "Paradol Chamber: 1\n",
      "Anderson: 1\n",
      "Clark Russell’s: 1\n",
      "Elias: 1\n",
      "Joseph: 1\n",
      "Freebody: 1\n",
      "Fareham: 1\n",
      "this John Openshaw: 1\n",
      "Sholtos: 1\n",
      "Dundee: 1\n",
      "Openshaw’s: 1\n",
      "Tragedy Near: 1\n",
      "Waterloo Bridge: 1\n",
      "Waterloo Station: 1\n",
      "S. H. for J.\n",
      "O.”: 1\n",
      "James Calhoun: 1\n",
      "Barque: 1\n",
      "Calhoun: 1\n",
      "Goodwins: 1\n",
      "Savannah: 1\n",
      "L. S.: 1\n",
      "Elias Whitney: 1\n",
      "Principal: 1\n",
      "De Quincey’s: 1\n",
      "Kate Whitney: 1\n",
      "Hitherto: 1\n",
      "Upper Swandam Lane: 1\n",
      "I.\n",
      ": 1\n",
      "Whitney: 1\n",
      "Paul’s Wharf: 1\n",
      "St. Clair’s: 1\n",
      "Near Lee: 1\n",
      "St.\n",
      "Clair: 1\n",
      "St. Clair: 1\n",
      "Lascar: 1\n",
      "Barton: 1\n",
      "mousseline de\n",
      "soie: 1\n",
      "Neville St.: 1\n",
      "Henry Bakers: 1\n",
      "jewel-case: 1\n",
      "Catherine\n",
      "Cusack: 1\n",
      "Star: 1\n",
      "Pall Mall: 1\n",
      "Bloomsbury: 1\n",
      "Holborn: 1\n",
      "Breckinridge: 1\n",
      "Bill: 1\n",
      "Cocksure: 1\n",
      "the King of Proosia: 1\n",
      "John Robinson: 1\n",
      "Ryder: 1\n",
      "Catherine Cusack: 1\n",
      "Cusack: 1\n",
      "Jem’s bird: 1\n",
      "Saxon: 1\n",
      "Roylotts of Stoke Moran: 1\n",
      "Berkshire: 1\n",
      "Honoria Westphail: 1\n",
      "Helen: 1\n",
      "Harrow: 1\n",
      "thick bell-rope: 1\n",
      "Grimesby\n",
      "Roylott: 1\n",
      "Warburton: 1\n",
      "Victor Hatherley: 1\n",
      "Lysander\n",
      "Stark: 1\n",
      "Reading: 1\n",
      "Fritz: 1\n",
      "Jeremiah Hayling: 1\n",
      "Becher’s: 1\n",
      "Lord St.\n",
      "Simon: 1\n",
      "ROBERT ST: 1\n",
      "Robert Walsingham de Vere St. Simon: 1\n",
      "Tudor: 1\n",
      "St. Simon: 1\n",
      "Aloysius\n",
      "Doran: 1\n",
      "Eustace: 1\n",
      "Lady Clara St. Simon: 1\n",
      "Lady Alicia Whittington: 1\n",
      "Miss Doran: 1\n",
      "Trafalgar Square: 1\n",
      "F. H. M.’ Now: 1\n",
      "glass sherry: 1\n",
      "Lady St.\n",
      "Simon: 1\n",
      "gras: 1\n",
      "Francis Hay Moulton: 1\n",
      "McQuire: 1\n",
      "Frank’s: 1\n",
      "Lord St.\n",
      "Simon’s: 1\n",
      "Francis H. Moulton: 1\n",
      "XI: 1\n",
      "Alexander Holder: 1\n",
      "Lucy: 1\n",
      "I.: 1\n",
      "Mary Holder: 1\n",
      "Francis Prosper: 1\n",
      "George\n",
      "Burnwell: 1\n",
      "George: 1\n",
      "George’s: 1\n",
      "the King of Bohemia: 1\n",
      "Pshaw: 1\n",
      "Westaway’s: 1\n",
      "Winchester: 1\n",
      "Hampshire: 1\n",
      "Edward: 1\n",
      "Jephro: 1\n",
      "tawny: 1\n",
      "Alice Rucastle: 1\n",
      "Copper Beeches: 1\n",
      "Walsall: 1\n"
     ]
    }
   ],
   "source": [
    "# Your code here: extract PERSON entities using spaCy NER\n",
    "# You'll need: import spacy, nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(story_text)\n",
    "characters = sorted({\n",
    "    ent.text for ent in doc.ents if ent.label_ == 'PERSON'\n",
    "})\n",
    "\n",
    "character_counts = Counter(\n",
    "    ent.text for ent in doc.ents if ent.label_ == \"PERSON\"\n",
    ")\n",
    "for name, count in character_counts.most_common():\n",
    "    print(f\"{name}: {count}\")\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/characters.txt\", \"w\") as f:\n",
    "#     for name in your_character_list:\n",
    "#         f.write(f\"{name}\\n\")\n",
    "\n",
    "with open(\"output/characters.txt\", \"w\") as f:\n",
    "    for name, count in character_counts.most_common():\n",
    "        f.write(f\"{name}\\t{count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Story Locations\n",
    "\n",
    "> \"Where does the story take place?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract location entities (GPE and LOC).\n",
    "\n",
    "**Hint:** Filter entities where `ent.label_` is 'GPE' (geopolitical entity) or 'LOC' (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc3f8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract GPE and LOC entities using spaCy NER\n",
    "\n",
    "locations = sorted({\n",
    "    ent.text\n",
    "    for ent in doc.ents\n",
    "    if ent.label_ in (\"GPE\", \"LOC\")\n",
    "})\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/locations.txt\", \"w\") as f:\n",
    "#     for place in your_locations_list:\n",
    "#         f.write(f\"{place}\\n\")\n",
    "\n",
    "with open(\"output/locations.txt\", \"w\") as f:\n",
    "    for place in locations:\n",
    "        f.write(f\"{place}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Wilson's Business\n",
    "\n",
    "> \"What is Wilson's business?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's business.\n",
    "\n",
    "**Hint:** Create a TF-IDF vectorizer, fit it on the 3 sections, then transform your query using the same vectorizer (`.transform()`, not `.fit_transform()` - you want to use the vocabulary learned from the sections). Find which section has the highest cosine similarity and read it to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "# You'll need: from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#              from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = re.findall(r'[^.!?]+[.!?]', story_text)\n",
    "query = \"What is Wilson's business?\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf = vectorizer.fit_transform(sentences + [query])\n",
    "\n",
    "similarities = cosine_similarity(tfidf[-1], tfidf[:-1])[0]\n",
    "best_sentence = sentences[similarities.argmax()].strip()\n",
    "\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/business.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's business is: ...\")\n",
    "\n",
    "with open(\"output/business.txt\", \"w\") as f:\n",
    "    f.write(f\"Wilson's business is: {best_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85452bf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Wilson's Work Routine\n",
    "\n",
    "> \"What is Wilson's daily work routine for the League?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's work routine.\n",
    "\n",
    "**Hint:** Similar to Question 4 - use TF-IDF to find the section that best matches your query about work routine. The answer includes what Wilson had to do and what eventually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddea14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/routine.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's work routine: ...\\n\")\n",
    "#     f.write(\"What happened: ...\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
